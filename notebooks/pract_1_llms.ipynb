{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - LLMs and Prompts\n",
    "This notebook walks through testing an LLM using the primary prompt templates used in llama-index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document - text file\n",
    "with open(\n",
    "    \"/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-llamaindex/ref_llamaindex_bottomsup/docs/getting_started/starter_example.md\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template = Prompt(\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = Prompt(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT QA TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you can follow the steps outlined in the context information provided:\n",
      "\n",
      "1. Clone the LlamaIndex repository by running the following command in your terminal:\n",
      "```bash\n",
      "$ git clone https://github.com/jerryjliu/llama_index.git\n",
      "```\n",
      "\n",
      "2. Navigate to the cloned repository and verify its contents:\n",
      "```bash\n",
      "$ cd llama_index\n",
      "$ ls\n",
      "```\n",
      "\n",
      "3. Navigate to the `examples` folder within the repository:\n",
      "```bash\n",
      "$ cd examples/paul_graham_essay\n",
      "```\n",
      "\n",
      "Following these steps will allow you to download and access the LlamaIndex examples.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I installa llama-index?\"\n",
    "prompt = text_qa_template.format(\n",
    "    context_str=text, query_str=question\n",
    ")  # note text was loaded from the markdown file, check the setup section above\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index, you can follow these steps:\n",
      "\n",
      "1. Download the LlamaIndex repository by cloning the repo using the command `git clone https://github.com/jerryjliu/llama_index.git`.\n",
      "2. Navigate to the `examples/paul_graham_essay` folder within the cloned repository.\n",
      "3. Create a new `.py` file and add the following Python code:\n",
      "\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```\n",
      "\n",
      "4. This code builds an index over the documents in the `data` folder (which contains the essay text).\n",
      "5. You can then run a query on the index using the following code:\n",
      "\n",
      "```python\n",
      "query_engine = index.as_query_engine()\n",
      "response = query_engine.query(\"What did the author do growing up?\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "6. Running this query should return a response similar to `The author wrote short stories and tried to program on an IBM 1401.`\n"
     ]
    }
   ],
   "source": [
    "# ask next question\n",
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFINED TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "index.storage_context.persist()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(\n",
    "    context_msg=text, query_str=question, existing_answer=existing_answer\n",
    ")\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: To create an index, you can follow these general steps:\n",
      "\n",
      "1. Define the purpose of the index: Determine what specific data or information you want to organize and make easily accessible through the index.\n",
      "\n",
      "2. Identify the items to be indexed: Decide what items, topics, or keywords you want to include in the index.\n",
      "\n",
      "3. Organize the index entries: Group related items together and create a hierarchical structure if needed.\n",
      "\n",
      "4. Assign index entries: Assign appropriate index entries to each item based on its content or relevance.\n",
      "\n",
      "5. Format the index: Decide on the format of the index, such as alphabetical order, chronological order, or any other logical sequence.\n",
      "\n",
      "6. Create the index: Compile all the index entries and format them according to your chosen structure and format.\n",
      "\n",
      "7. Review and revise: Review the index for accuracy, completeness, and usability. Make any necessary revisions to improve the index.\n",
      "\n",
      "8. Publish the index: Once you are satisfied with the index, publish it in the appropriate format (e.g., printed book, online document, website).\n",
      "\n",
      "These steps can be adapted based on the specific requirements of your project or the type of index you are creating.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=\"You are a helpful QA chatbot that can answer questions about llama-index.\",\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
